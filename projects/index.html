<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Projects — Feroz Khan</title>
  <link rel="stylesheet" href="../assets/style.css" />
</head>
<body>
  <div class="wrap">
    <div class="nav">
      <div class="left"><a href="../">← Home</a></div>
      <div class="navlinks">
        <a href="https://drive.google.com/drive/folders/1r66grJ7YAS4UBYX32mdgBNJfvxTGNN2G?usp=sharing">Resume</a>
        <a href="https://github.com/ferozk0333">GitHub</a>
      </div>
    </div>

    <section class="hero">
      <h1>Project Descriptions</h1>
      <p class="sub">Short and readable write-ups.</p>
    </section>



    
    <div class="card" id="project-1">
      <h3>Fine-Tuning CNN Architectures Using Transfer Learning</h3>
    
      <p class="meta">
        <strong>Tech:</strong> PyTorch, Transfer Learning, Optuna, GPU  
        · <strong>Dataset:</strong> Fashion MNIST  
        · <strong>Outcome:</strong> 91% → 95.5% Accuracy
      </p>
    
      <p><strong>Context</strong><br/>
      Baseline CNN plateaued at 91% accuracy; required stronger generalization on limited labeled data.</p>
    
      <p><strong>Approach</strong></p>
      <ul>
        <li>Converted 28×28 grayscale → 3-channel RGB; applied Resize / CenterCrop / Normalize transforms</li>
        <li>Built custom PyTorch Dataset + DataLoader ETL pipeline</li>
        <li>Loaded pretrained VGG16 (ImageNet) and ResNET models, froze backbone, replaced classifier head with custom head</li>
        <li>Trained with CrossEntropy + Adam using K-Fold CV and Bayesian Optuna tuning</li>
      </ul>

      <p><strong>Architecture</strong></p>
      <img 
        src="../assets/images/vgg16-transfer-learning.png" 
        alt="VGG16 Transfer Learning Architecture Diagram"
        style="width:100%; max-width:750px; border-radius:8px; margin:10px 0;"
      />

      <p><strong>Result</strong><br/>
      Improved accuracy to 95.5% (+4.5%), reduced overfitting via weight decay tuning, and achieved faster convergence vs training from scratch for a domain-specific, smaller dataset.</p>
    
      <p>
        <a href="https://github.com/ferozk0333/Transfer-Learning-Fine-Tuning-Using-PyTorch">Repository</a>
      </p>
    </div>
    
    <div style="height:14px"></div>




    <div class="card" id="project-2">
      <h3>Sentiment Analysis for Healthcare Domain with NLP & MLOps</h3>
    
      <p class="meta">
        <strong>Tech:</strong> Python, Scikit-learn, MLflow, SQL  
        · <strong>Dataset:</strong> Govt. of India survey data (~15,000 anonymized patient text entries) + Kaggle sentiment dataset  
        · <strong>Outcome:</strong> 80% Accuracy · 0.84 ROC-AUC · 12% F1 disparity reduction
      </p>
    
      <p><strong>Context</strong><br/>
      Built an NLP classification system to analyze rural patient text and assist urban therapists with sentiment signals and recurring thought categorization across 7 predefined mental health concern classes.</p>
    
      <p><strong>Approach</strong></p>
      <ul>
        <li><strong>Pipeline:</strong> Standardized preprocessing (tokenization, stopword removal, TF-IDF), strict train/validation split to prevent leakage, normalized SQL schema for storing predictions and metadata.</li>
        <li><strong>Database:</strong> Designed and normalized SQL database to store text entries, timespamp, sensitive PII information. Wrote queries with JOINs. </li>        
        <li><strong>Models:</strong> Multinomial Naive Bayes (baseline) vs Logistic Regression (self-trained model).</li>
        <li><strong>Experiments:</strong> MLflow-based tracking of hyperparameters, metrics, model artifacts, and version comparisons.</li>
      </ul>
      <p><strong>Architecture</strong></p>
      <img 
        src="../assets/images/MentalHealth.png" 
        alt="Architecture Diagram"
        style="width:100%; max-width:750px; border-radius:8px; margin:10px 0;"
      />
      <p><strong>Results</strong><br/>
      Logistic Regression improved accuracy from 72% (Naive Bayes baseline) to <strong>94.65%</strong>, increased ROC-AUC to <strong>0.84</strong>, and reduced cross-demographic F1 disparity by <strong>12%</strong> via class rebalancing and threshold calibration. Improved recall for high-risk categories by 9%.</p>
    
      <p>
        <a href="https://github.com/ferozk0333/Mental-Health-Analytics-using-NLP">Repository</a>
      </p>
    </div>
    <div style="height:14px"></div>




    
    <div class="card" id="project-3">
      <h3>Agentic RAG with Hallucination Filtering & Self-Reflection</h3>
    
      <p class="meta">
        <strong>Tech:</strong> LangGraph, LangChain, FAISS, OpenAI APIs, FastAPI, Docker, AWS  
        · <strong>Dataset:</strong> 5,000+ indexed documents + live web search  
        · <strong>Outcome:</strong> +15% GPT-Judge accuracy · 20% redundancy reduction
      </p>
    
      <p><strong>Context</strong><br/>
      Designed an agentic Retrieval-Augmented Generation (RAG) system to reduce hallucinations and improve factual grounding in multi-domain question answering.</p>
    
      <p><strong>Approach</strong></p>
      <ul>
        <li><strong>Routing:</strong> Query classification to vector DB (FAISS) or Tavily web search</li>
        <li><strong>Retrieval:</strong> Embedding pipeline with MMR + multi-query expansion to improve recall and reduce redundancy</li>
        <li><strong>Generation:</strong> Context-aware answer synthesis using OpenAI APIs</li>
        <li><strong>Validation:</strong> Binary hallucination grading (LLM + NLI) with adaptive self-reflection and query rewriting</li>
        <li><strong>Flow Control:</strong> LangGraph state machine for modular agent orchestration</li>
      </ul>
      <p><strong>Architecture</strong></p>
      <img 
        src="../assets/images/rag.png" 
        alt="Architecture Diagram"
        style="width:100%; max-width:750px; border-radius:8px; margin:10px 0;"
      />
      <p><strong>Metrics</strong><br/>
      Improved GPT-Judge factual accuracy by <strong>15%</strong> vs baseline RAG, reduced retrieval redundancy by <strong>20%</strong>, and decreased hallucination rate by <strong>18%</strong> in simulated evaluation.</p>
    
      <p><strong>Deployment & MLOps</strong><br/>
      Containerized with Docker, deployed on AWS EC2/SageMaker, CI/CD via GitHub Actions, and exposed scalable FastAPI endpoints with Pydantic validation.</p>
    
      <p><strong>Key Design Decisions</strong><br/>
      Adopted agentic routing for modularity, integrated automated hallucination grading for reliability, and prioritized retrieval diversity (MMR + multi-query) to improve grounding.</p>
    
      <p>
        <a href="https://github.com/ferozk0333/Agentic-RAG-with-Hallucination-Filtering-Self-Reflection">Repository</a>
      </p>
    </div>





    <div class="card" id="project-4">
      <h3>Fine-Tuning LLM with Quantization & LoRA</h3>
    
      <p class="meta">
        <strong>Tech:</strong> Hugging Face Transformers, PEFT, Datasets, PyTorch  
        · <strong>Base Model:</strong> Gemma-7B  
        · <strong>Dataset:</strong> 945K+ customer support tweet pairs (input → response)  
        · <strong>Outcome:</strong> +14% relevance · 60% lower memory · -35% training cost (est.)
      </p>
    
      <p><strong>Context</strong><br/>
      Fine-tuned Gemma-7B for customer support response generation using parameter-efficient LoRA adapters, optimized for lower-memory training and cost-effective inference.</p>
    
      <p><strong>Pipeline</strong></p>
      <ul>
        <li><strong>Data:</strong> Tokenized input/response pairs (max_length=512), added [PAD] token, and generated labels for supervised causal LM</li>
        <li><strong>Training:</strong> LoRA on attention projections (q_proj, v_proj) with r=16, α=32, dropout=0.1; AdamW + gradient accumulation</li>
        <li><strong>Efficiency:</strong> Loaded model in <strong>8-bit</strong> via bitsandbytes to enable training on smaller GPUs</li>
        <li><strong>Artifacts:</strong> Saved LoRA adapters and checkpoints for lightweight deployment</li>
      </ul>
    
      <p><strong>Results</strong><br/>
      On a 300-sample blind test set, improved response relevance by <strong>14%</strong> vs base Gemma-7B, increased win-rate to <strong>62%</strong> in pairwise preference eval, and reduced hallucinated/irrelevant replies by <strong>11%</strong>. 8-bit + LoRA reduced peak GPU memory by <strong>~60%</strong>, enabling deployment on lower-cost infrastructure.</p>
    
      <p><strong>Key Design Decisions</strong><br/>
      Chose LoRA for fast iteration and small artifact size, 8-bit quantization for hardware efficiency, and targeted attention modules (q/v) for best quality–cost tradeoff.</p>
    
      <p>
        <a href="https://github.com/ferozk0333/Fine-Tuning-Gemma-with-LoRA-for-Customer-Support-Automation">Repository</a>
      </p>
    </div>







    <div class="card" id="project-5">
      <h3>End-to-End Sales BI Pipeline & Dashboard</h3>
    
      <p class="meta">
        <strong>Tech:</strong> MySQL, Power Query, Airflow, Power BI  
        · <strong>Dataset:</strong> ~150K OLTP sales transactions  
        · <strong>Outcome:</strong> 20% reduction in ad-hoc analysis · Real-time KPI visibility
      </p>
    
      <p><strong>Context</strong><br/>
      Built a scalable BI pipeline to transform raw transactional sales data into structured analytics-ready datasets, enabling stakeholders to monitor revenue, customer performance, and product trends in real time.</p>
    
      <p><strong>Architecture & Pipeline</strong></p>
      <ul>
        <li><strong>Data Model:</strong> Designed star schema with fact (transactions) and dimensions (customer, product, market, date)</li>
        <li><strong>ETL:</strong> Cleaned and transformed ~150K records using SQL + Power Query (handled duplicates, nulls, invalid entries, format standardization)</li>
        <li><strong>Orchestration:</strong> Migrated workflow to Airflow for automated scheduling and improved scalability</li>
        <li><strong>Integration:</strong> Connected directly to MySQL for centralized, near real-time data access</li>
      </ul>
    
      <p><strong>Analytics & Visualization</strong></p>
      <ul>
        <li>Built KPI dashboards tracking revenue, sales quantity, top customers, and product contribution</li>
        <li>Implemented <strong>Decomposition Trees</strong> to analyze drivers of order profit and identify underperforming segments</li>
        <li>Enabled drill-down views by region, product category, and customer segment</li>
      </ul>

      <p><strong>Dashboard Preview</strong></p>
      <img 
        src="../assets/images/powerbi1.png" 
        alt="Architecture Diagram"
        style="width:100%; max-width:750px; border-radius:8px; margin:10px 0;"
      />
      <img 
        src="../assets/images/powerbi2.png" 
        alt="Architecture Diagram"
        style="width:100%; max-width:750px; border-radius:8px; margin:10px 0;"
      />
      <p><strong>Results</strong><br/>
      Reduced manual reporting and ad-hoc analysis by ~20%, improved decision latency, and provided actionable insights into regional revenue trends and product performance.</p>
    
      <p>
        <a href="https://github.com/ferozk0333/Sales_Insight_Using_PowerBI_MySQL">Repository</a>
      </p>
    </div>



    <div class="card" id="project-6">
      <h3>End-to-End Regression ML System with AWS CI/CD</h3>
    
      <p class="meta">
        <strong>Tech:</strong> Regression, Scikit-learn, Pandas, Flask, Docker, GitHub Actions, AWS (ECR, EC2)  
        · <strong>Dataset:</strong> Student performance tabular dataset (~1,000 rows, mixed numeric + categorical)  
        · <strong>Outcome:</strong> MAE 3.8 · RMSE 5.1 · R² 0.89 (test)
      </p>
    
      <p><strong>Context</strong><br/>
      Built a production-style regression system to predict student exam scores from demographic and academic signals, packaged as a web app and deployed on AWS with CI/CD.</p>
    
      <p><strong>Pipeline & Modeling</strong></p>
      <ul>
        <li><strong>Ingestion:</strong> Raw CSV train/test split with logging for traceability</li>
        <li><strong>Transformation:</strong> Missing/duplicate handling, One-Hot Encoding for categorical features, StandardScaler for numeric features</li>
        <li><strong>Feature Selection:</strong> Correlation pruning + L1 regularization to reduce multicollinearity and remove noisy features</li>
        <li><strong>Modeling:</strong> Ridge/Lasso/ElasticNet regression with hyperparameter tuning (CV) to control overfitting</li>
        <li><strong>Artifacts:</strong> Persisted preprocessor + best model as versioned pickle artifacts for consistent inference</li>
      </ul>

      <p><strong>High Level Workflow</strong></p>
      <img 
        src="../assets/images/regression.png" 
        alt="Architecture Diagram"
        style="width:100%; max-width:750px; border-radius:8px; margin:10px 0;"
      />
      <p><strong>Results</strong><br/>
      Achieved <strong>MAE 3.8</strong> and <strong>RMSE 5.1</strong> on held-out test data (R² <strong>0.89</strong>). Regularization reduced train–test gap and improved stability vs an unregularized baseline (MAE 5.0, RMSE 6.6).</p>
    
      <p><strong>Deployment & MLOps</strong><br/>
      Containerized Flask inference service, pushed images to <strong>AWS ECR</strong>, deployed on <strong>EC2</strong>, and automated build/test/deploy with <strong>GitHub Actions</strong>. Designed the system so model + preprocessor artifacts can be swapped without rewriting the app.</p>
    
      <p><strong>Key Design Decisions</strong><br/>
      Used L1/L2 regularization for generalization under correlated features, enforced consistent preprocessing via saved pipelines, and chose Docker + ECR for repeatable deployments and environment parity.</p>
    
      <p>
        <a href="https://github.com/ferozk0333/Machine_Learning_based_Test_Score_Predictor_with_AWS_CI-CD_Deployment">Repository</a>
      </p>
    </div>





    

  </div>
</body>
</html>


