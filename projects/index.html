<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Projects — Feroz Khan</title>
  <link rel="stylesheet" href="../assets/style.css" />
</head>
<body>
  <div class="wrap">
    <div class="nav">
      <div class="left"><a href="../">← Home</a></div>
      <div class="navlinks">
        <a href="https://drive.google.com/drive/folders/1r66grJ7YAS4UBYX32mdgBNJfvxTGNN2G?usp=sharing">Resume</a>
        <a href="https://github.com/ferozk0333">GitHub</a>
      </div>
    </div>

    <section class="hero">
      <h1>Project Descriptions</h1>
      <p class="sub">Short and readable write-ups.</p>
    </section>



    
    <div class="card" id="project-1">
      <h3>Fine-Tuning CNN Architectures Using Transfer Learning</h3>
    
      <p class="meta">
        <strong>Tech:</strong> PyTorch, Transfer Learning, Optuna, GPU  
        · <strong>Dataset:</strong> Fashion MNIST  
        · <strong>Outcome:</strong> 91% → 95.5% Accuracy
      </p>
    
      <p><strong>Context</strong><br/>
      Baseline CNN plateaued at 91% accuracy; required stronger generalization on limited labeled data.</p>
    
      <p><strong>Approach</strong></p>
      <ul>
        <li>Converted 28×28 grayscale → 3-channel RGB; applied Resize / CenterCrop / Normalize transforms</li>
        <li>Built custom PyTorch Dataset + DataLoader ETL pipeline</li>
        <li>Loaded pretrained VGG16 (ImageNet) and ResNET models, froze backbone, replaced classifier head with custom head</li>
        <li>Trained with CrossEntropy + Adam using K-Fold CV and Bayesian Optuna tuning</li>
      </ul>

      <p><strong>Architecture</strong></p>
      <img 
        src="../assets/images/vgg16-transfer-learning.png" 
        alt="VGG16 Transfer Learning Architecture Diagram"
        style="width:100%; max-width:750px; border-radius:8px; margin:10px 0;"
      />

      <p><strong>Result</strong><br/>
      Improved accuracy to 95.5% (+4.5%), reduced overfitting via weight decay tuning, and achieved faster convergence vs training from scratch for a domain-specific, smaller dataset.</p>
    
      <p>
        <a href="https://github.com/ferozk0333/Transfer-Learning-Fine-Tuning-Using-PyTorch">Repository</a>
      </p>
    </div>
    
    <div style="height:14px"></div>




    <div class="card" id="project-2">
      <h3>Sentiment Analysis for Healthcare Domain with NLP & MLOps</h3>
    
      <p class="meta">
        <strong>Tech:</strong> Python, Scikit-learn, MLflow, SQL  
        · <strong>Dataset:</strong> Govt. of India survey data (~15,000 anonymized patient text entries) + Kaggle sentiment dataset  
        · <strong>Outcome:</strong> 80% Accuracy · 0.84 ROC-AUC · 12% F1 disparity reduction
      </p>
    
      <p><strong>Context</strong><br/>
      Built an NLP classification system to analyze rural patient text and assist urban therapists with sentiment signals and recurring thought categorization across 7 predefined mental health concern classes.</p>
    
      <p><strong>Approach</strong></p>
      <ul>
        <li><strong>Pipeline:</strong> Standardized preprocessing (tokenization, stopword removal, TF-IDF), strict train/validation split to prevent leakage, normalized SQL schema for storing predictions and metadata.</li>
        <li><strong>Database:</strong> Designed and normalized SQL database to store text entries, timespamp, sensitive PII information. Wrote queries with JOINs. </li>        
        <li><strong>Models:</strong> Multinomial Naive Bayes (baseline) vs Logistic Regression (self-trained model).</li>
        <li><strong>Experiments:</strong> MLflow-based tracking of hyperparameters, metrics, model artifacts, and version comparisons.</li>
      </ul>
      <p><strong>Architecture</strong></p>
      <img 
        src="../assets/images/MentalHealth.png" 
        alt="Architecture Diagram"
        style="width:100%; max-width:750px; border-radius:8px; margin:10px 0;"
      />
      <p><strong>Results</strong><br/>
      Logistic Regression improved accuracy from 72% (Naive Bayes baseline) to <strong>94.65%</strong>, increased ROC-AUC to <strong>0.84</strong>, and reduced cross-demographic F1 disparity by <strong>12%</strong> via class rebalancing and threshold calibration. Improved recall for high-risk categories by 9%.</p>
    
      <p>
        <a href="https://github.com/ferozk0333/Mental-Health-Analytics-using-NLP">Repository</a>
      </p>
    </div>
    <div style="height:14px"></div>




    
    <div class="card" id="project-3">
      <h3>Agentic RAG with Hallucination Filtering & Self-Reflection</h3>
    
      <p class="meta">
        <strong>Tech:</strong> LangGraph, LangChain, FAISS, OpenAI APIs, FastAPI, Docker, AWS  
        · <strong>Dataset:</strong> 5,000+ indexed documents + live web search  
        · <strong>Outcome:</strong> +15% GPT-Judge accuracy · 20% redundancy reduction
      </p>
    
      <p><strong>Context</strong><br/>
      Designed an agentic Retrieval-Augmented Generation (RAG) system to reduce hallucinations and improve factual grounding in multi-domain question answering.</p>
    
      <p><strong>Approach</strong></p>
      <ul>
        <li><strong>Routing:</strong> Query classification to vector DB (FAISS) or Tavily web search</li>
        <li><strong>Retrieval:</strong> Embedding pipeline with MMR + multi-query expansion to improve recall and reduce redundancy</li>
        <li><strong>Generation:</strong> Context-aware answer synthesis using OpenAI APIs</li>
        <li><strong>Validation:</strong> Binary hallucination grading (LLM + NLI) with adaptive self-reflection and query rewriting</li>
        <li><strong>Flow Control:</strong> LangGraph state machine for modular agent orchestration</li>
      </ul>
      <p><strong>Architecture</strong></p>
      <img 
        src="../assets/images/rag.png" 
        alt="Architecture Diagram"
        style="width:100%; max-width:750px; border-radius:8px; margin:10px 0;"
      />
      <p><strong>Metrics</strong><br/>
      Improved GPT-Judge factual accuracy by <strong>15%</strong> vs baseline RAG, reduced retrieval redundancy by <strong>20%</strong>, and decreased hallucination rate by <strong>18%</strong> in simulated evaluation.</p>
    
      <p><strong>Deployment & MLOps</strong><br/>
      Containerized with Docker, deployed on AWS EC2/SageMaker, CI/CD via GitHub Actions, and exposed scalable FastAPI endpoints with Pydantic validation.</p>
    
      <p><strong>Key Design Decisions</strong><br/>
      Adopted agentic routing for modularity, integrated automated hallucination grading for reliability, and prioritized retrieval diversity (MMR + multi-query) to improve grounding.</p>
    
      <p>
        <a href="#">Repository</a>
      </p>
    </div>



    
    <div class="card" id="project-2">
      <h3>Project 2: Title</h3>
      <p class="meta">Tech: … · Outcome: …</p>
      <p>
        Problem: …<br/>
        Approach: …<br/>
        Result: …
      </p>
      <p><a href="https://github.com/your-username/project2">Repository</a></p>
    </div>
  </div>
</body>
</html>


